{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import inspect\n",
        "import traceback\n",
        "from xpms_helper.model.data_schema import DatasetFormat, DatasetConvertor\n",
        "from xpms_helper.model import model_utils\n",
        "from sklearn.metrics.scorer import SCORERS\n",
        "from xpms_helper.model.train_info import TrainInfo\n",
        "from xpms_helper.model.model_utils import calculate_metrics\n",
        "from xpms_helper.model import dataset_utils, model_utils\n",
        "import xgboost as xgb\n",
        "from xpms_file_storage.file_handler import XpmsResourceFactory, XpmsResource, LocalResource\n",
        "from xpms_storage.utils import get_env\n",
        "NAMESPACE = get_env(\"NAMESPACE\", \"claims-audit\", False)\n",
        "AMAZON_AWS_BUCKET = get_env(\"AMAZON_AWS_BUCKET\", \"xpms-ca-test\", False)\n",
        "\n",
        "def train(datasets,config):\n",
        "    train_info = {\"name\" : \"XGB\"}\n",
        "    result_dataset = run(datasets,config)\n",
        "    return train_info, result_dataset\n",
        "\n",
        "def get_params(config):\n",
        "    try:\n",
        "        params = config[\"algorithm\"][\"configuration\"]\n",
        "    except:\n",
        "        params = dict()\n",
        "    # this comes here\n",
        "\n",
        "    if \"additional_params\" in config:\n",
        "        for k in [\"learning_rates\",\"num_boost_round\"]:\n",
        "            params[k] = config[\"additional_params\"][k]\n",
        "        if \"params\" in  config[\"additional_params\"]:\n",
        "            params[\"params\"].update(config[\"additional_params\"][\"params\"])\n",
        "    return params\n",
        "\n",
        "def run_model(config, model_obj, X, en_classes, de_classes):\n",
        "    class_indexes = {}\n",
        "    for index in range(0, len(en_classes)):\n",
        "        class_indexes[en_classes[index]] = index\n",
        "    predictions = []\n",
        "    # params = get_params(config)[\"params\"]\n",
        "    params = config[\"params\"]\n",
        "    predictions_raw = model_obj.predict(X)\n",
        "    if params[\"objective\"] in [\"binary:logistic\"]:\n",
        "        for val in predictions_raw:\n",
        "            row = [(1-val), val]\n",
        "            predictions.append(row)\n",
        "    elif params[\"objective\"] in [\"multi:softmax\"]:\n",
        "        for val in predictions_raw:\n",
        "            row = [0] * len(en_classes)\n",
        "            row[class_indexes[val]] = 1\n",
        "            predictions.append(row)\n",
        "    elif params[\"objective\"] == \"multi:softprob\":\n",
        "        predictions = predictions_raw\n",
        "    else:\n",
        "        raise Exception(\"unsupported objective parameter\")\n",
        "\n",
        "    result_df = pd.DataFrame(data=predictions, columns=de_classes)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def run(datasets, config, caching=None):\n",
        "    dataset = DatasetConvertor.convert(datasets, DatasetFormat.DATA_FRAME, None)\n",
        "    run_df = dataset[\"value\"]\n",
        "    target_column = \"cas_stus_dscr\"\n",
        "    X = run_df.loc[:, run_df.columns != target_column]\n",
        "\n",
        "    dtest = xgb.DMatrix(data=X)\n",
        "\n",
        "    file_name = \"audit_model_core_0.9.pkl\"\n",
        "\n",
        "    model_obj = model_utils.load(file_name=file_name, config=config, caching=caching)\n",
        "\n",
        "    encoder = SimpleEncoder()\n",
        "\n",
        "    encoder.fit(config, [], exec_mode=\"run\")\n",
        "\n",
        "    params = get_params(config)\n",
        "    params[\"params\"] = params.get(\"params\", {})\n",
        "    params[\"params\"][\"objective\"] = \"binary:logistic\"\n",
        "\n",
        "    result_df = run_model(params, model_obj, dtest, encoder.encoded_labels(), encoder.labels())\n",
        "\n",
        "    result_dataset = {\"value\": result_df, \"data_format\": \"data_frame\"}\n",
        "    return result_dataset\n",
        "\n",
        "\n",
        "def evaluate(datasets, config, caching=None):\n",
        "    dataset = DatasetConvertor.convert(datasets, DatasetFormat.DATA_FRAME, None)\n",
        "    if \"scorers\" in config:\n",
        "        scorers = config[\"scorers\"]\n",
        "    else:\n",
        "        scorers = [\"accuracy\"]\n",
        "    eval_df = dataset[\"value\"]\n",
        "    target_colum = \"cas_stus_dscr\"\n",
        "\n",
        "    y = eval_df[target_colum]\n",
        "\n",
        "    model_output = run(datasets, config, caching=caching)\n",
        "    # get prediction columns\n",
        "    y_pred = model_output[\"value\"].idxmax(axis=1).values\n",
        "    score = calculate_metrics(dataset[\"value\"], scorers, y, y_pred, config)\n",
        "    return score, model_output\n",
        "\n",
        "def retrain(datasets, config, caching=False):\n",
        "    dataset = DatasetConvertor.convert(datasets, DatasetFormat.DATA_FRAME, None)\n",
        "    train_df = dataset[\"value\"]\n",
        "    target_column = \"cas_stus_dscr\"\n",
        "\n",
        "    x = train_df.loc[:, train_df.columns != target_column]\n",
        "    y = train_df[target_column]\n",
        "    caf_cfe = train_df[target_column].value_counts().to_dict()\n",
        "\n",
        "    encoder = SimpleEncoder()\n",
        "    y = encoder.fit(config, y, exec_mode=\"retrain\")\n",
        "    dtrain = xgb.DMatrix(x, label=y)\n",
        "\n",
        "    params = get_params(config)\n",
        "    params[\"params\"] = params.get(\"params\", {})\n",
        "    # if params[\"params\"][\"objective\"] in [\"multi:softmax\",\"multi:softprob\"]:\n",
        "    #     params[\"params\"][\"num_class\"] = len(set(y))\n",
        "\n",
        "\n",
        "    params[\"params\"][\"objective\"] = \"binary:logistic\"\n",
        "    params[\"params\"][\"scale_pos_weight\"] = caf_cfe[0] / caf_cfe[1]\n",
        "    params[\"params\"][\"process_type\"] = \"update\"\n",
        "    params[\"params\"][\"updater\"] = \"refresh\"\n",
        "    params[\"params\"][\"refresh_leaf\"] = True\n",
        "\n",
        "    #load previous model\n",
        "    file_name = \"{0}.pkl\".format(\"audit_model_core_0.9\")\n",
        "    model_obj = model_utils.load(file_name=file_name, config=config, caching=caching)\n",
        "    model_obj = xgb.train( dtrain=dtrain, xgb_model=model_obj, **params)\n",
        "    file_name = \"{}.pkl\".format(\"audit_model_core_0.9\")\n",
        "    model_utils.save(file_name=file_name, obj=model_obj, config=config)\n",
        "\n",
        "    result_df = run_model(params, model_obj, dtrain, encoder.encoded_labels(), encoder.labels())\n",
        "    full_dataset = dataset_utils.update_dataset({\"0\":datasets}, result_df)\n",
        "\n",
        "    data = datasets['value']\n",
        "    train_info = TrainInfo(\n",
        "        **{\"name\": \"\", \"path\": config[\"src_dir\"], \"params\": params, \"classes\": encoder.labels(), \"rec\": data.shape[0], \"col\": data.shape[1],\n",
        "           \"dep_var\": target_column}).as_json()\n",
        "\n",
        "    config[\"algorithm\"] = dict(path=config[\"src_dir\"])\n",
        "    result_dataset = {\"value\": full_dataset, \"data_format\": \"data_frame\",\"target_column\":target_column, \"predicted_classes\": encoder.labels()}\n",
        "    return train_info, result_dataset\n",
        "\n",
        "class SimpleEncoder:\n",
        "\n",
        "    def labels(self):\n",
        "        return self.labels_list\n",
        "\n",
        "    def encoded_labels(self):\n",
        "        return list(range(0, len(self.labels())))\n",
        "\n",
        "    def defcode(self, config):\n",
        "        pass\n",
        "\n",
        "    def fit(self, config, Y, exec_mode = \"train\"):\n",
        "        if exec_mode == \"train\":\n",
        "            labels_list = list(set(Y))\n",
        "            label_map = dict()\n",
        "            for i in range(0, len(labels_list)):\n",
        "                label_map[labels_list[i]] = i\n",
        "            label_coding = {\n",
        "                \"labels_list\": labels_list,\n",
        "                \"label_map\": label_map\n",
        "            }\n",
        "            csv_minio_urn = \"minio://{}/label_encoder/audit_target_label_encoding\".format(AMAZON_AWS_BUCKET)\n",
        "            local_csv_path = \"/tmp/audit_target_label_encoding\"\n",
        "            minio_resource = XpmsResource.get(urn=csv_minio_urn)\n",
        "            pickle.dump(label_coding,open(local_csv_path,\"wb\"))\n",
        "            local_res = LocalResource(key=local_csv_path)\n",
        "            local_res.copy(minio_resource)\n",
        "            # model_utils.save(\"label_coding\", label_coding, config)\n",
        "        elif exec_mode in [\"run\", \"eval\"]:\n",
        "\n",
        "            file_path = \"minio://{}/label_encoder/audit_target_label_encoding\".format(AMAZON_AWS_BUCKET)\n",
        "            local_pkl_path = \"/tmp/audit_target_label_encoding\"\n",
        "            minio_resource = XpmsResource.get(urn=file_path)\n",
        "            local_res = LocalResource(key=local_pkl_path)\n",
        "            minio_resource.copy(local_res)\n",
        "            label_coding = pickle.load(open(local_pkl_path, \"rb\"))\n",
        "            # label_coding = model_utils.load(\"label_coding\", config)\n",
        "            label_map = label_coding[\"label_map\"]\n",
        "            labels_list = label_coding[\"labels_list\"]\n",
        "\n",
        "        elif exec_mode == \"retrain\":\n",
        "            file_path = \"minio://{}/label_encoder/audit_target_label_encoding\".format(AMAZON_AWS_BUCKET)\n",
        "            local_pkl_path = \"/tmp/audit_target_label_encoding\"\n",
        "            minio_resource = XpmsResource.get(urn=file_path)\n",
        "            local_res = LocalResource(key=local_pkl_path)\n",
        "            minio_resource.copy(local_res)\n",
        "            label_coding = pickle.load(open(local_pkl_path, \"rb\"))\n",
        "\n",
        "            # label_coding = model_utils.load(\"label_coding\", config)\n",
        "            labels_list = list(set(Y))\n",
        "            for label in labels_list:\n",
        "                if label not in label_coding[\"labels_list\"]:\n",
        "                    label_coding[\"label_map\"][label] = len(label_coding[\"labels_list\"])\n",
        "                    label_coding[\"labels_list\"].append(label)\n",
        "            model_utils.save(\"label_coding\", label_coding, config)\n",
        "            labels_list = label_coding[\"labels_list\"]\n",
        "            label_map = label_coding[\"label_map\"]\n",
        "        else:\n",
        "            raise Exception(\"exec mode {} is not handled in the model runner script\".format(exec_mode))\n",
        "\n",
        "        y_ = []\n",
        "        for label in Y:\n",
        "            y_.append(label_map[label])\n",
        "\n",
        "        self.label_map = label_map\n",
        "        self.labels_list = labels_list\n",
        "\n",
        "        return y_\n",
        "\n",
        "def test_template():\n",
        "    config={}\n",
        "    config[\"storage\"] = \"local\"\n",
        "    config[\"src_dir\"] = os.getcwd()\n",
        "    dataset_obj = json.load(open(os.path.join(os.getcwd(),\"datasets_obj/dataset_obj.json\")))\n",
        "    dataset_format = dataset_obj[\"data_format\"]\n",
        "    if dataset_format != \"list\":\n",
        "        dataset_obj[\"value\"] = LocalResource(key= os.path.join(os.getcwd(),\"datasets\")).urn\n",
        "    train(dataset_obj,config)\n",
        "    run(dataset_obj,config)\n",
        "    evaluate(dataset_obj,config)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}