{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.options.display.max_columns = None\n",
        "from xpms_file_storage.file_handler import XpmsResourceFactory, XpmsResource, LocalResource\n",
        "from xpms_storage.db_handler import DBProvider\n",
        "from xpms_storage.utils import get_env\n",
        "\n",
        "df_info = []\n",
        "\n",
        "\n",
        "# NUM_CLAIMS_PER_FILE\n",
        "\n",
        "\n",
        "def split_hma_batches_record(config=None, **objects):\n",
        "    retries = 3\n",
        "    counter = 1\n",
        "    while counter < retries:\n",
        "        try:\n",
        "            NAMESPACE = get_env(\"NAMESPACE\", \"claims-audit\", False)\n",
        "            AMAZON_AWS_BUCKET = get_env(\"AMAZON_AWS_BUCKET\", \"xpms-ca-test\", False)\n",
        "            ENV_DATABASE = get_env('DATABASE_PARAPHRASE', None, True)\n",
        "\n",
        "            NUM_CLAIMS_PER_FILES = json.loads(config['NUM_CLAIMS_PER_FILES'])\n",
        "\n",
        "            NUM_CLAIMS_PER_FILE = int(NUM_CLAIMS_PER_FILES)\n",
        "            file_path = objects[\"document\"][0][\"metadata\"][\"properties\"][\"file_metadata\"][\"file_path\"]\n",
        "            local_csv_path = \"/tmp/\" + objects[\"document\"][0][\"metadata\"][\"properties\"][\"filename\"]\n",
        "            minio_resource = XpmsResource.get(urn=file_path)\n",
        "            local_res = LocalResource(key=local_csv_path)\n",
        "            minio_resource.copy(local_res)\n",
        "            # split file name from config\n",
        "            file_name = objects[\"document\"][0][\"metadata\"][\"properties\"][\"filename\"].split(\".\")[0]\n",
        "            batch_name = config[\"context\"][\"batch_name\"]\n",
        "            threshold = config[\"context\"][\"threshold\"]\n",
        "\n",
        "            # three funtion for split big file to small one\n",
        "\n",
        "            def file_sequence(batch_name, local_csv_path_df, threshold):\n",
        "                NAMESPACE = get_env(\"NAMESPACE\", \"claims-audit\", False)\n",
        "                AMAZON_AWS_BUCKET = get_env(\"AMAZON_AWS_BUCKET\", \"xpms-ca-test\", False)\n",
        "                ENV_DATABASE = get_env('DATABASE_PARAPHRASE', None, True)\n",
        "                # file_name = file_name.split(\".\")[-2]\n",
        "                local_res = LocalResource(key=local_csv_path_df)\n",
        "                csv_minio_urn = \"minio://{0}/\".format(\n",
        "                    AMAZON_AWS_BUCKET) + \"claimsaudit-ingestfiles/archive/split-batches-input-csv-pending/\" + local_res.filename\n",
        "                minio_resource = XpmsResource.get(urn=csv_minio_urn)\n",
        "\n",
        "                local_res.copy(minio_resource)\n",
        "                batch_ob = {\n",
        "\n",
        "                    \"batch_name\": batch_name,\n",
        "                    \"file_name_chunk\": local_res.filename,\n",
        "                    \"threshold\": threshold,\n",
        "                    \"status\": \"pending\"\n",
        "\n",
        "                }\n",
        "                try:\n",
        "                    db = DBProvider.get_instance(db_name=ENV_DATABASE)\n",
        "                    s = db.insert(table='batch_metadata_chunk', rows=batch_ob)\n",
        "                except Exception as e:\n",
        "                    return 'e is ' + str(e)\n",
        "\n",
        "            def create_csv_with_num_claims(file_name, batch_name, remaining_dataframes, current_df, unique_claim_count,\n",
        "                                           threshold):\n",
        "                \"\"\" Create csv files from the provided chunks of dataframes by satisfying\n",
        "                the number of claims per file\n",
        "\n",
        "                Args:\n",
        "                    remaining_dataframes ([list of df]): list of dataframes which are to be\n",
        "                    written to the csv file\n",
        "                    current_df ([df]): dataframe of the current chunk of file\n",
        "                    unique_claim_count ([int]): maintains the number of unique claims throughout\n",
        "                    the input file\n",
        "\n",
        "                Returns:\n",
        "                    remaining_dataframes ([list of df]): list of dataframes which are to be\n",
        "                    written to the csv file\n",
        "                    unique_claim_count ([int]): maintains the number of unique claims throughout\n",
        "                    the input file\n",
        "                    :param unique_claim_count:\n",
        "                    :param current_df:\n",
        "                    :param remaining_dataframes:\n",
        "                    :param batch_name:\n",
        "                    :param file_name:\n",
        "                \"\"\"\n",
        "\n",
        "                remaining_dataframes.append(current_df)\n",
        "\n",
        "                df_ = pd.concat(remaining_dataframes)\n",
        "                claims = list(df_.CLAIM_NUMBER_Mask.unique())\n",
        "                current_unique_claims_count = len(claims)\n",
        "\n",
        "                if current_unique_claims_count >= NUM_CLAIMS_PER_FILE:\n",
        "\n",
        "                    remaining_dataframes = []\n",
        "\n",
        "                    remaining_ids = current_unique_claims_count % NUM_CLAIMS_PER_FILE\n",
        "                    claim_sets = [claims[s:s + NUM_CLAIMS_PER_FILE]\n",
        "                                  for s in range(0, current_unique_claims_count, NUM_CLAIMS_PER_FILE)]\n",
        "                    if remaining_ids != 0:\n",
        "                        last_claims_df = df_[df_['CLAIM_NUMBER_Mask'].isin(claim_sets[-1])]\n",
        "                        remaining_dataframes.append(last_claims_df)\n",
        "                        claim_sets.pop(-1)\n",
        "\n",
        "                    for _ in claim_sets:\n",
        "                        unique_claim_count += len(_)\n",
        "                        df_sub = df_[df_['CLAIM_NUMBER_Mask'].isin(_)]\n",
        "                        df_name = \"{0}_{1}.csv\".format(batch_name, unique_claim_count + 1)\n",
        "                        # filename = \"fixed_claims_file_{}.csv\".format(unique_claim_count + 1)\n",
        "                        path = r\"/tmp/\" + df_name\n",
        "                        df_sub.to_csv(path, index=False)\n",
        "                        file_sequence(batch_name, path, threshold)\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "                return remaining_dataframes, unique_claim_count\n",
        "\n",
        "            # functions of create chunks files\n",
        "            def create_chunks(file_name, batch_name, chunkfile_location, threshold):\n",
        "                \"\"\"Split the input file in multiple files making sure following conditions are satified:\n",
        "                1. one claim number does not occur in multiple files\n",
        "                2. each file has maximum of NUM_CLAIMS_PER_FILE\n",
        "                3. no claims are missed\n",
        "\n",
        "                Args:\n",
        "                    chunkfile_location ([string]): input file location\n",
        "                    :param chunkfile_location:\n",
        "                    :param batch_name:\n",
        "                    :param file_name:\n",
        "                \"\"\"\n",
        "                claim_rows = 0\n",
        "                remaining_dfs = []\n",
        "                chunksize = 1000\n",
        "                unique_claim_count = 0\n",
        "                for chunk_num, chunk in enumerate(pd.read_csv(chunkfile_location, chunksize=chunksize)):\n",
        "                    #         if chunk_num%10==0: print(chunk_num)\n",
        "                    if chunk_num == 0:\n",
        "                        initial_df = chunk\n",
        "                    else:\n",
        "                        last_claim_initial = initial_df.CLAIM_NUMBER_Mask.iloc[-1]\n",
        "                        first_claim_current = chunk.CLAIM_NUMBER_Mask.iloc[0]\n",
        "                        if last_claim_initial == first_claim_current:\n",
        "                            claim_to_move = initial_df[initial_df['CLAIM_NUMBER_Mask']\n",
        "                                                       == last_claim_initial]\n",
        "                            initial_df = initial_df[initial_df['CLAIM_NUMBER_Mask']\n",
        "                                                    != last_claim_initial]\n",
        "                            if initial_df.shape[0] != 0:\n",
        "                                remaining_dfs, unique_claim_count = create_csv_with_num_claims(file_name, batch_name,\n",
        "                                                                                               remaining_dfs,\n",
        "                                                                                               initial_df,\n",
        "                                                                                               unique_claim_count,\n",
        "                                                                                               threshold)\n",
        "                                claim_rows += initial_df.shape[0]\n",
        "                            chunk = claim_to_move.append(chunk)\n",
        "                            initial_df = chunk\n",
        "                        else:\n",
        "                            remaining_dfs, unique_claim_count = create_csv_with_num_claims(file_name, batch_name,\n",
        "                                                                                           remaining_dfs,\n",
        "                                                                                           initial_df,\n",
        "                                                                                           unique_claim_count,\n",
        "                                                                                           threshold)\n",
        "                            claim_rows += initial_df.shape[0]\n",
        "                            initial_df = chunk\n",
        "                if chunk.shape[0] != chunksize:\n",
        "                    remaining_dfs, unique_claim_count = create_csv_with_num_claims(file_name, batch_name,\n",
        "                                                                                   remaining_dfs,\n",
        "                                                                                   chunk,\n",
        "                                                                                   unique_claim_count, threshold)\n",
        "                    if len(remaining_dfs) != 0:\n",
        "                        rem_df = pd.concat(remaining_dfs)\n",
        "                        # filename = \"fixed_claims_file_{}.csv\".format(unique_claim_count)\n",
        "                        df_name = \"{0}_{1}.csv\".format(batch_name, unique_claim_count)\n",
        "                        path = r\"/tmp/\" + df_name\n",
        "                        rem_df.to_csv(path, index=False)\n",
        "                        file_sequence(batch_name, path, threshold)\n",
        "                        current_unique_claim_ids = rem_df.CLAIM_NUMBER_Mask.unique()\n",
        "                        unique_claim_count += len(current_unique_claim_ids)\n",
        "                    claim_rows += chunk.shape[0]\n",
        "                print(\"claim rows \", claim_rows)\n",
        "                print(\"unique_claim_count \", unique_claim_count)\n",
        "                return unique_claim_count\n",
        "\n",
        "            unique_claim = create_chunks(file_name, batch_name, local_csv_path, threshold)\n",
        "\n",
        "            if unique_claim % NUM_CLAIMS_PER_FILE == 0:\n",
        "                no_chunk = unique_claim // NUM_CLAIMS_PER_FILE\n",
        "            else:\n",
        "                no_chunk = unique_claim // NUM_CLAIMS_PER_FILE + 1\n",
        "\n",
        "            filter_ob = {\"batch_name\": batch_name}\n",
        "            update_ob = {\n",
        "                \"no_of_chunk\": no_chunk,\n",
        "                \"batch_volume\": unique_claim\n",
        "            }\n",
        "            try:\n",
        "                db = DBProvider.get_instance(db_name=ENV_DATABASE)\n",
        "                s = db.update(table='batch_metadata', update_obj=update_ob, filter_obj=filter_ob)\n",
        "            except Exception as e:\n",
        "                return 'e is ' + str(e)\n",
        "\n",
        "            return objects\n",
        "\n",
        "        except Exception as e:\n",
        "            counter += 1\n",
        "        finally:\n",
        "            local_res.delete()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}