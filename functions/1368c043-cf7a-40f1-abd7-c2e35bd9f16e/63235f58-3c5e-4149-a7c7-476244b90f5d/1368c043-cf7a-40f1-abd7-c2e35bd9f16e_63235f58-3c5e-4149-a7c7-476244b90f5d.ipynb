{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import datetime\n",
        "from xpms_file_storage.file_handler import XpmsResourceFactory, XpmsResource, LocalResource\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xpms_storage.db_handler import DBProvider\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import requests\n",
        "from xpms_storage.utils import get_env\n",
        "\n",
        "\n",
        "def hma_postprocess_audit_model(config=None, **obj):\n",
        "    retries = 3\n",
        "    counter = 1\n",
        "    while counter < retries:\n",
        "        try:\n",
        "            ENV_DATABASE = get_env('DATABASE_PARAPHRASE', None, True)\n",
        "            AMAZON_AWS_BUCKET = get_env(\"AMAZON_AWS_BUCKET\", \"xpms-ca-test\", False)\n",
        "            batch_name = config['context']['batch_name']\n",
        "            start_time = int(config['context']['start_time'])\n",
        "            threshold = float(config['context']['threshold'])\n",
        "            file_name = config['context']['input_file_name']\n",
        "            converted_start_time = datetime.utcfromtimestamp(start_time)\n",
        "            file_path = config[\"context\"][\"source_file_path\"]\n",
        "            local_csv_path = config[\"context\"][\"local_source_file_path\"]\n",
        "            xr1 = XpmsResource()\n",
        "            minio_resource = xr1.get(urn=file_path)\n",
        "            local_res = LocalResource(key=local_csv_path)\n",
        "            minio_resource.copy(local_res)\n",
        "            df = pd.read_csv(local_csv_path)\n",
        "\n",
        "            # Result 1\n",
        "            result_path = obj['result_path']\n",
        "            local_csv_path = \"/tmp/vmAuditTest.csv\"\n",
        "            xr1 = XpmsResource()\n",
        "            minio_resource = xr1.get(urn=result_path)\n",
        "            local_res_1 = LocalResource(key=local_csv_path)\n",
        "            minio_resource.copy(local_res_1)\n",
        "            df1 = pd.read_csv(local_csv_path)\n",
        "            df1 = df1.apply(lambda x: (round(x * 100, 2)) / 100)\n",
        "            df1.rename(columns={\"0\": \"CFE\", \"1\": \"CAF\"}, inplace=True)\n",
        "\n",
        "            final_df = pd.concat([df, df1.drop(df1.columns[0], axis=1)], axis=1)\n",
        "            aggregated_df = final_df.groupby('CLAIM_NUMBER_Mask').agg(\n",
        "                lambda x: list(x) if len(set(x)) == 1 else list(x)).replace(\n",
        "                'nan', '').reset_index()\n",
        "\n",
        "            lcols = ['CAF', 'CFE']\n",
        "            aggregated_df['index_choice'] = aggregated_df['CFE'].apply(lambda x: x.index(max(x)))\n",
        "            for col in lcols:\n",
        "                aggregated_df[col + '_confidence'] = aggregated_df.apply(lambda row: row[col][row['index_choice']],\n",
        "                                                                         axis=1)\n",
        "            rename_cols = {\"CAF\": \"aggregated_caf\",\n",
        "                           \"CFE\": \"aggregated_cfe\",\n",
        "\n",
        "                           \"CAF_confidence\": \"CAF\",\n",
        "                           \"CFE_confidence\": \"CFE\",\n",
        "                           }\n",
        "            aggregated_df.rename(columns=rename_cols, inplace=True)\n",
        "\n",
        "            aggregated_df[\"AFV_PRV_CRG_AMT_1\"] = [sum(x) for x in aggregated_df[\"AFV_PRV_CRG_AMT_1\"]]\n",
        "\n",
        "            aggregated_df[\"Audit Result\"] = aggregated_df[\"CAF\"].apply(\n",
        "                lambda x: \"CAF\" if x >= float(threshold) / 100 else \"CFE\")\n",
        "\n",
        "            aggregated_df[\"system_recommended_result\"] = aggregated_df[[\"CAF\", \"CFE\"]].to_dict(orient='records')\n",
        "\n",
        "            final_df[\"Audit Result\"] = final_df['CLAIM_NUMBER_Mask'].map(\n",
        "                aggregated_df.set_index('CLAIM_NUMBER_Mask')['Audit Result'])\n",
        "\n",
        "            final_df[\"index_choice\"] = final_df['CLAIM_NUMBER_Mask'].map(\n",
        "                aggregated_df.set_index('CLAIM_NUMBER_Mask')['index_choice'])\n",
        "\n",
        "            aggregated_df_copy = aggregated_df.copy(deep=True)\n",
        "            final_df_copy = final_df.copy(deep=True)\n",
        "\n",
        "            rename_c = {\n",
        "                \"CLAIM_NUMBER_Mask\": \"CLAIM NUMBER\",\n",
        "                \"AFV_PRV_CRG_AMT_1\": \"TOT PROV CHARGE\",\n",
        "            }\n",
        "            aggregated_df_copy.rename(columns=rename_c, inplace=True)\n",
        "            # final_df_copy.rename(columns=rename_c,inplace=True)\n",
        "\n",
        "            aggregated_obj = json.loads(aggregated_df_copy.to_json(orient='records'))\n",
        "            line_level_obj = json.loads(final_df.to_json(orient='records'))\n",
        "\n",
        "            clean_ob_lst = [\n",
        "                {'batch_name': batch_name, 'data': item, 'start_time': start_time,\n",
        "                 \"converted_start_time\": converted_start_time,\n",
        "                 'threshold': float(threshold / 100),\n",
        "                 'file_name': file_name, 'flag': 'untrained'} for item in aggregated_obj]\n",
        "\n",
        "            line_level_lst = [\n",
        "                {'batch_name': batch_name, 'data': item, 'start_time': start_time,\n",
        "                 \"converted_start_time\": converted_start_time,\n",
        "                 'threshold': float(threshold / 100),\n",
        "                 'file_name': file_name, 'flag': 'untrained'} for item in line_level_obj]\n",
        "\n",
        "            try:\n",
        "                db = DBProvider.get_instance(db_name=ENV_DATABASE)\n",
        "                s1 = db.insert(table='claims_data', rows=clean_ob_lst)\n",
        "                s2 = db.insert(table='line_level_claims_data', rows=line_level_lst)\n",
        "                filter_ob = {'file_name_chunk': file_name}\n",
        "                update_ob = {\n",
        "                    \"$set\": {\n",
        "                        'audit_not_needed': aggregated_df_copy[aggregated_df_copy['CAF'] >= threshold / 100].shape[0],\n",
        "                        'audit_needed': aggregated_df_copy[aggregated_df_copy['CAF'] < threshold / 100].shape[0],\n",
        "                        \"status\": \"in-progress\"}}\n",
        "\n",
        "                s3 = db.update(table='batch_metadata_chunk', update_obj=update_ob, filter_obj=filter_ob)\n",
        "\n",
        "                if aggregated_df_copy[aggregated_df_copy['CAF'] < threshold / 100].shape[0] == 0:\n",
        "                    return {\n",
        "                        \"status\": \"completed\",\n",
        "                        \"claims_db_inserted\": s1,\n",
        "                        \"line_level_claims_data_inserted\": s3,\n",
        "                        \"batch_metadata_updated\": s2,\n",
        "                        \"dataset\": {\n",
        "                            \"data_format\": \"csv\",\n",
        "                            \"value\": \"na\"\n",
        "                        }\n",
        "                    }\n",
        "                else:\n",
        "                    cfe_df = final_df[final_df[\"Audit Result\"] == \"CFE\"]\n",
        "                    file_name = file_path.split('/')[-1]\n",
        "                    csv_minio_urn = \"minio://{}/ml_input/cfe_\".format(AMAZON_AWS_BUCKET) + file_name\n",
        "                    local_csv_path = \"/tmp/cfe_\" + file_name\n",
        "                    minio_resource = XpmsResource.get(urn=csv_minio_urn)\n",
        "                    cfe_df.to_csv(local_csv_path, index=False)\n",
        "                    local_res_2 = LocalResource(key=local_csv_path)\n",
        "                    local_res_2.copy(minio_resource)\n",
        "                    config[\"context\"][\"cfe_source_file\"] = csv_minio_urn\n",
        "                    local_res_2.delete()\n",
        "                    return {\n",
        "                        \"dataset\": {\n",
        "                            \"data_format\": \"csv\",\n",
        "                            \"value\": csv_minio_urn\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    \"status\": \"failed\",\n",
        "                    \"claims_db_inserted\": False,\n",
        "                    \"line_level_claims_data_inserted\": False,\n",
        "                    \"batch_metadata_updated\": False,\n",
        "                    \"error_message\": str(e)\n",
        "                }\n",
        "        except Exception as e:\n",
        "            counter += 1\n",
        "        finally:\n",
        "            local_res.delete()\n",
        "            local_res_1.delete()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}