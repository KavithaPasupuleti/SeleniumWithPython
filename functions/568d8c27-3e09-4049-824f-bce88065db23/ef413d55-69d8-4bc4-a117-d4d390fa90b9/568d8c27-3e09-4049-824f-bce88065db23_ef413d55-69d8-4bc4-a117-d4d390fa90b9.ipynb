{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from xpms_storage.db_handler import DBProvider\n",
        "from xpms_storage.utils import get_env\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from xpms_file_storage.file_handler import XpmsResourceFactory, XpmsResource, LocalResource\n",
        "\n",
        "\n",
        "def hma_split_files_status(config=None, **objects):\n",
        "\n",
        "    ENV_DATABASE = get_env('DATABASE_PARAPHRASE', None, True)\n",
        "    BE_URL = get_env('CLAIMS_AUDIT_APIS_URL', None, True)\n",
        "    AMAZON_AWS_BUCKET = get_env(\"AMAZON_AWS_BUCKET\", \"xpms-ca-test\", False)\n",
        "    try:\n",
        "        db = DBProvider.get_instance(db_name=ENV_DATABASE)\n",
        "\n",
        "        aggregate = [\n",
        "            {\n",
        "                \"$match\": {\n",
        "                    \"no_of_chunk\": {\"$exists\": True},\n",
        "                    \"status\": \"to-do\"\n",
        "                }\n",
        "\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    \"batch_name\": \"$batch_name\",\n",
        "                    \"no_of_chunk\": \"$no_of_chunk\",\n",
        "                    \"file_name\": \"$file_name\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        batch_metadata = db.find(table='batch_metadata', aggregate=aggregate)\n",
        "        batch_name_chunk_map = {datum['batch_name']: [datum['no_of_chunk'], datum['file_name']] for datum in\n",
        "                                batch_metadata}\n",
        "\n",
        "        aggregate_1 = [\n",
        "            {\"$match\": {'batch_name': {\"$in\": list(batch_name_chunk_map.keys())},\n",
        "                        \"status\": \"completed\"}},\n",
        "            {\n",
        "                \"$group\": {\n",
        "                    \"_id\": \"$batch_name\",\n",
        "                    \"total\": {\"$sum\": 1},\n",
        "                    \"audit_needed\": {'$sum': '$audit_needed'},\n",
        "                    \"audit_not_needed\": {'$sum': '$audit_not_needed'}\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\n",
        "                    '_id': 0,\n",
        "                    'batch_name': \"$_id\",\n",
        "                    \"total\": 1,\n",
        "                    'audit_needed': 1,\n",
        "                    'audit_not_needed': 1\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        batch_metadata_chunk = db.find(table='batch_metadata_chunk', aggregate=aggregate_1)\n",
        "        for obj in batch_metadata_chunk:\n",
        "            batch_name = obj['batch_name']\n",
        "            file_name = batch_name_chunk_map[batch_name][1]\n",
        "            if obj['total'] == batch_name_chunk_map[batch_name][0]:\n",
        "                filter_ob = {'batch_name': batch_name}\n",
        "                update_ob = {\n",
        "                    \"status\": \"in-progress\",\n",
        "                    \"audit_needed\": obj['audit_needed'],\n",
        "                    \"audit_not_needed\": obj['audit_not_needed'],\n",
        "                    \"batch_end_date\": int(time.time())\n",
        "                }\n",
        "                s = db.update(table='batch_metadata', update_obj=update_ob, filter_obj=filter_ob)\n",
        "                notification = {\n",
        "                    \"group\": \"batch_status\",\n",
        "                    \"message\": {\n",
        "                        \"body\": f'{batch_name} is in-progress.',\n",
        "                        \"status\": \"info\",\n",
        "                        \"title\": batch_name,\n",
        "                        \"icon\": \"processing\"\n",
        "                    },\n",
        "                    \"metadata\": {\n",
        "                        \"batch_name\": batch_name,\n",
        "                        \"current_status\": \"in-progress\",\n",
        "                        \"previous_status\": \"to-do\"\n",
        "                    },\n",
        "                    \"created_timestamp\": int(time.time())\n",
        "                }\n",
        "\n",
        "                db.insert(table='notifications', rows=[notification])\n",
        "                file_path = \"minio://{0}/claimsaudit-ingestfiles/archive/split-input-csv-inprogress\".format(\n",
        "                    AMAZON_AWS_BUCKET)\n",
        "                xr = XpmsResource()\n",
        "                minio_resource = xr.get(urn=file_path)\n",
        "                if minio_resource.exists():\n",
        "                    all_files_list = minio_resource.list()\n",
        "                    files_list = [(path.filename) for path in all_files_list if \".csv\" in path.fullpath]\n",
        "                    if len(files_list) == 0:\n",
        "\n",
        "                        return {\n",
        "                            \"file_path\": \"na\"\n",
        "                        }\n",
        "\n",
        "                    else:\n",
        "                        backup_path = \"minio://{0}/claimsaudit-ingestfiles/archive/split-batches_completed\".format(\n",
        "                            AMAZON_AWS_BUCKET)\n",
        "                        for file_name_big in files_list:\n",
        "                            if file_name == file_name_big:\n",
        "                                xrm = XpmsResource()\n",
        "                                mr = xrm.get(urn=file_path + '/' + file_name_big)\n",
        "\n",
        "                                backup_urn = backup_path + '/' + file_name_big\n",
        "                                backup_rm = XpmsResource()\n",
        "                                backup_mr = backup_rm.get(urn=backup_urn)\n",
        "                                mr.copy(backup_mr)\n",
        "                                mr.delete()\n",
        "                celery_batch_url = f\"https://{BE_URL}/celery/batch-ingested-calculation\"\n",
        "\n",
        "                payload = {}\n",
        "                headers = {\n",
        "                    'Content-Type': 'application/json'\n",
        "                }\n",
        "\n",
        "                response = requests.request(\"GET\", celery_batch_url, headers=headers, data=payload)\n",
        "\n",
        "                print(response.text.encode('utf8'))\n",
        "\n",
        "                url = f'https://{BE_URL}/send_notification'\n",
        "                headers = {\n",
        "                    'Content-Type': 'application/json'\n",
        "                }\n",
        "\n",
        "                requests.request(\"POST\", url, headers=headers, data=json.dumps(notification, default=str))\n",
        "\n",
        "    except Exception as e:\n",
        "        return 'e is ' + str(e)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}